{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape data from WOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from pyvirtualdisplay import Display\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait as wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "for x in soup.find_all('td', 'goto'):\n",
    "    print (x.text.split(' ')[-2])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "for x in soup.find_all('a', 'smallV110 snowplow-full-record'):\n",
    "    url = 'http://apps.webofknowledge.com' + x['href']\n",
    "    \n",
    "short = url.split('page')[0]\n",
    "\n",
    "for x in soup.find_all('td', 'goto'):\n",
    "    pn = x.text.split(' ')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sn = []\n",
    "tk = []\n",
    "tt = []\n",
    "\n",
    "for xm in range(1, int(pn)+1):\n",
    "    for ym in range(1, 51):\n",
    "        time.sleep(5)\n",
    "        nurl = short + 'page=' + str(xm)+ '&doc=' + str((xm - 1)*50 + ym)\n",
    "        \n",
    "        driver.get(nurl)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        keys = []\n",
    "        for x in soup.find_all('div', 'block-record-info'):\n",
    "            for y in x.find_all('div', 'title3'):\n",
    "                if 'word' in y.text:\n",
    "                    for z in x.find_all('p', 'FR_field'):\n",
    "                        keys.append(z.text) \n",
    "        tk.append(keys)\n",
    "        \n",
    "        nn = ''\n",
    "        for x in soup.find_all('div', 'block-record-info'):\n",
    "            for y in x.find_all('p', 'FR_field'):\n",
    "                if 'DOI' in y.text:\n",
    "                    nn = y.text\n",
    "        sn.append(nn)\n",
    "        \n",
    "        title = ''\n",
    "        for x in soup.find_all('div', 'title'):\n",
    "            title = x.text\n",
    "        tt.append(title)\n",
    "        \n",
    "    np.save('n_kw_' + str(xm) + '.npy', tk)\n",
    "    np.save('n_sn_' + str(xm) + '.npy', sn)\n",
    "    np.save('n_title_' + str(xm) + '.npy', tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sn = []\n",
    "# tk = []\n",
    "# tt = []\n",
    "\n",
    "for xm in range(36, 37):\n",
    "    for ym in range(19, 51):\n",
    "        time.sleep(5)\n",
    "        nurl = short + 'page=' + str(xm)+ '&doc=' + str((xm - 1)*50 + ym)\n",
    "        \n",
    "        driver.get(nurl)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        keys = []\n",
    "        for x in soup.find_all('div', 'block-record-info'):\n",
    "            for y in x.find_all('div', 'title3'):\n",
    "                if 'word' in y.text:\n",
    "                    for z in x.find_all('p', 'FR_field'):\n",
    "                        keys.append(z.text) \n",
    "        tk.append(keys)\n",
    "        \n",
    "        nn = ''\n",
    "        for x in soup.find_all('div', 'block-record-info'):\n",
    "            for y in x.find_all('p', 'FR_field'):\n",
    "                if 'DOI' in y.text:\n",
    "                    nn = y.text\n",
    "        sn.append(nn)\n",
    "        \n",
    "        title = ''\n",
    "        for x in soup.find_all('div', 'title'):\n",
    "            title = x.text\n",
    "        tt.append(title)\n",
    "        \n",
    "    np.save('kw_' + str(xm) + '.npy', tk)\n",
    "    np.save('sn_' + str(xm) + '.npy', sn)\n",
    "    np.save('title_' + str(xm) + '.npy', tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "\n",
    "for x in range(1, 2001):\n",
    "    for y in range(1, 51):\n",
    "        nurl = short + 'page=' + str(x)+ '&doc=' + str((x - 1)*50 + y)\n",
    "        urls.append(nurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4052, 4052)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tk), len(sn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 19)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    np.save('kw_' + str(xm) + '.npy', tk)\n",
    "    np.save('sn_' + str(xm) + '.npy', sn)\n",
    "    np.save('title_' + str(xm) + '.npy', tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Author Keywords:Crowdshipping; On demand economy; Urban freight transport; City logistics; Discrete choice models; Stated preference\n",
      "\n",
      "\n",
      "KeyWords Plus:CHOICE; ACCEPTABILITY; PREFERENCES; CITY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "keys = []\n",
    "\n",
    "for x in soup.find_all('div', 'block-record-info'):\n",
    "    for y in x.find_all('div', 'title3'):\n",
    "        if 'word' in y.text:\n",
    "            for z in x.find_all('p', 'FR_field'):\n",
    "#                 for o in z.find_all('a'):\n",
    "#                     print(o.text.title())  \n",
    "                    \n",
    "                print (z.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"block-record-info\"> <div class=\"title3\">Journal Information</div> <ul> </ul> <ul> <span id=\"links_isi_product_2\"></span> </ul></div>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sn = []\n",
    "\n",
    "for x in soup.find_all('div', 'block-record-info'):\n",
    "    for y in x.find_all('p', 'FR_field'):\n",
    "        if 'DOI' in y.text:\n",
    "            sn.append(y.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nDOI:10.1016/j.dib.2018.12.003']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nDOI:\\n10.1186/s12544-019-0352-x\\n',\n",
       " '\\nDOI:\\n10.1016/j.eswa.2019.01.001\\n',\n",
       " '\\nDOI:10.1016/j.dib.2018.12.003',\n",
       " '\\nDOI:\\n10.1016/j.envint.2019.01.044\\n',\n",
       " '\\nDOI:\\n10.1016/j.envint.2019.01.061\\n',\n",
       " '\\nDOI:\\n10.1016/j.envint.2019.01.053\\n',\n",
       " '\\nDOI:\\n10.1016/j.envint.2019.01.036\\n',\n",
       " '\\nDOI:\\n10.1016/j.envint.2019.01.075\\n',\n",
       " '\\nDOI:\\n10.1016/j.envint.2019.01.077\\n']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Altered cord blood mitochondrial DNA content and pregnancy lead exposure in the PROGRESS cohort\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in soup.find_all('div', 'title'):\n",
    "    print (x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "for x in soup.find_all('a', 'smallV110 snowplow-full-record'):\n",
    "    url = 'http://apps.webofknowledge.com' + x['href']\n",
    "    \n",
    "short = url.split('page')[0]\n",
    "\n",
    "for x in soup.find_all('td', 'goto'):\n",
    "    pn = x.text.split(' ')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sn = []\n",
    "tk = []\n",
    "tt = []\n",
    "urls = []\n",
    "\n",
    "for xm in range(1, 2):\n",
    "    for ym in range(1, 51):\n",
    "        time.sleep(5)\n",
    "        nurl = short + 'page=' + str(xm)+ '&doc=' + str((xm - 1)*50 + ym)\n",
    "        urls.append(nurl)\n",
    "        \n",
    "        driver.get(nurl)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        keys = []\n",
    "        for x in soup.find_all('div', 'block-record-info'):\n",
    "            for y in x.find_all('div', 'title3'):\n",
    "                if 'word' in y.text:\n",
    "                    for z in x.find_all('p', 'FR_field'):\n",
    "                        keys.append(z.text) \n",
    "        tk.append(keys)\n",
    "        \n",
    "        nn = ''\n",
    "        for x in soup.find_all('div', 'block-record-info'):\n",
    "            for y in x.find_all('p', 'FR_field'):\n",
    "                if 'DOI' in y.text:\n",
    "                    nn = y.text\n",
    "        sn.append(nn)\n",
    "        \n",
    "        title = ''\n",
    "        for x in soup.find_all('div', 'title'):\n",
    "            title = x.text\n",
    "        tt.append(title)\n",
    "        \n",
    "    np.save('hot_n_kw_' + str(xm) + '.npy', tk)\n",
    "    np.save('hot_n_sn_' + str(xm) + '.npy', sn)\n",
    "    np.save('hot_n_title_' + str(xm) + '.npy', tt)\n",
    "    np.save('hot_n_url_' + str(xm) + '.npy', urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import inflect\n",
    "p = inflect.engine()\n",
    "kw = np.load('kw_81.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4052"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kw = list(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ol = []\n",
    "\n",
    "for x in range(len(kw)):\n",
    "    \n",
    "    words = kw[x]\n",
    "    for w in words:\n",
    "        s = (w.split(':')[1]).split(';')\n",
    "        for m in s:\n",
    "            nw = p.singular_noun(m.lower().strip('\\n').strip().replace('-', ' '))\n",
    "            if nw:\n",
    "                ol.append(nw)\n",
    "            else:\n",
    "                ol.append(m.lower().strip('\\n').strip().replace('-', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ul = list(set(ol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13352"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = C.Counter(ol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = []\n",
    "c = []\n",
    "\n",
    "for x in counter.most_common():\n",
    "    n.append(x[0])\n",
    "    c.append(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d['keyword'] = n\n",
    "d['count'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# d[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d[:50].to_csv('city_50_keywords.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = n[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = []\n",
    "\n",
    "for x in range(len(kw)):\n",
    "    \n",
    "    l = []\n",
    "    words = kw[x]\n",
    "    for w in words:\n",
    "        s = (w.split(':')[1]).split(';')\n",
    "        for m in s:\n",
    "            nw = p.singular_noun(m.lower().strip('\\n').strip().replace('-', ' '))\n",
    "\n",
    "            if nw:\n",
    "                t = nw\n",
    "            else:\n",
    "                t = m.lower().strip('\\n').strip().replace('-', ' ')\n",
    "            \n",
    "            try:\n",
    "                l.append(keys.index(t))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    links.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlinks = []\n",
    "\n",
    "for x in links:\n",
    "    for i in range(len(x) - 1):\n",
    "        for j in range(i + 1, len(x)):\n",
    "            nlinks.append([x[i], x[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.Graph() \n",
    "for x in range(50):\n",
    "    G.add_node(x)\n",
    "    \n",
    "# G.remove_node(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in nlinks:\n",
    "    G.add_edge(x[0], x[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.draw(G, with_labels=True)\n",
    "# plt.savefig('city_50_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix = np.zeros((50, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix = pd.DataFrame(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in nlinks:\n",
    "    matrix[x[0]][x[1]] += 1\n",
    "    matrix[x[1]][x[0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix.to_csv('city_top_50_matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get top articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "al = np.load('hot_title_1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "al = al[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "al = list(al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "al = [x.strip('\\n') for x in al]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame()\n",
    "out['rank'] = [x+1 for x in range(10)] \n",
    "out['article'] = al\n",
    "out.to_csv('city_hot_articles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.similarity('france', 'spain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
